{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook, we will explore the flexibility behind Azure AI Inference. This is the [library](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-inference-readme?view=azure-python-preview) from Azure, which allows us to run inference against a wide range of AI model deployments - both in Azure and, as we will see in this notebook, in other places as well.\n",
    "\n",
    "It is available for Python and for .NET - in this notebook, we will focus on the Python version. To begin with, we need to install the `azure.ai.inference` package. You can find the necessary dependencies in the accompanying `requirements.txt` file.\n",
    "\n",
    "You will need to set the following environment variables:\n",
    " * for the first example: `AZURE_OPENAI_RESOURCE` and `AZURE_OPENAI_KEY`\n",
    " * for the second example: `AZURE_AI_PROJECT` and `AZURE_AI_KEY`\n",
    " * the third example does not require any environment variables but a localhost Foundry Local server running on port 65431"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to define a general task for our models. It will be a sample health problem classification, where the model will be asked to categorize user's input into one of four possible classes:\n",
    " - `doctor_required` - if the user should see a doctor immediately\n",
    " - `pharmacist_required` - if the user should see a pharmacist - for problems that can be solved with over-the-counter drugs\n",
    " - `rest_required` - if the user should rest and does not need professional help\n",
    " - `unknown` - if the model is not sure about the classification\n",
    "\n",
    "![](images/classification.excalidraw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"You are a medical classification engine for health conditions. Classify the prompt into into one of the following possible treatment options: 'doctor_required' (serious condition), 'pharmacist_required' (light condition) or 'rest_required' (general tiredness). If you cannot classify the prompt, output 'unknown'. \n",
    "Only respond with the single word classification. Do not produce any additional output.\n",
    "\n",
    "# Examples:\n",
    "User: \"I did not sleep well.\" Assistant: \"rest_required\"\n",
    "User: \"I chopped off my arm.\" Assistant: \"doctor_required\"\n",
    "User: \"I am sneezing\" Assistant: \"pharmacist_required\"\n",
    "\n",
    "# Task\n",
    "User: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need a set of sample inputs to the model, and the expected outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_inputs = [\n",
    "    \"I'm tired.\", # rest_required\n",
    "    \"I'm bleeding from my eyes.\", # doctor_required\n",
    "    \"I have a running nose.\" # pharmacist_required\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference code is very simple - we will call the `complete` method on the inference client, and indicate that we are interested in the streaming of the response. This way, we can process the response as it comes in, and not wait for the whole response to be ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(client: ChatCompletionsClient):\n",
    "    for user_input in user_inputs:\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"{instruction}{user_input} Assistant: \"\n",
    "        }]\n",
    "        print(f\"{user_input} -> \", end=\"\")\n",
    "        stream = client.complete(\n",
    "            messages=messages,\n",
    "            stream=True\n",
    "        )\n",
    "        for chunk in stream:\n",
    "            if chunk.choices and chunk.choices[0].delta.content:\n",
    "                print(chunk.choices[0].delta.content, end=\"\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first example shows using the inference client against an Azure OpenAI endpoint. In this case, three arguments are mandatory: \n",
    " * an endpoint URL in the form of `https://<resouce-name>.openai.azure.com/openai/deployments/<deployment-name>` \n",
    " * the credential to access it (could be either the key or the integrated Azure SDK authentication)\n",
    " * the API version (this is mandatory in Azure OpenAI API access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_RESOURCE = os.environ[\"AZURE_OPENAI_RESOURCE\"]\n",
    "AZURE_OPENAI_KEY = os.environ[\"AZURE_OPENAI_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ChatCompletionsClient(\n",
    "    endpoint=f\"https://{AZURE_OPENAI_RESOURCE}.openai.azure.com/openai/deployments/gpt-4.1/\",\n",
    "    credential=AzureKeyCredential(AZURE_OPENAI_KEY),\n",
    "    api_version=\"2024-06-01\",\n",
    ")\n",
    "\n",
    "print(\" * AZURE OPENAI (GPT 4.1) * \")\n",
    "run_inference(client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next example shows using the client against Azure AI Foundry model deployment. The prerequisite here is to have a model deployed as standard deployment - the relevant instructions can be [found here](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/deployments-overview#how-should-i-think-about-deployment-options).\n",
    "\n",
    "The two pieces of information needed to connect to such model are:\n",
    " * an endpoint URL in the form of `https://<azure-ai-project>.services.ai.azure.com` \n",
    " * the credential to access it (could be either the key or the integrated Azure SDK authentication)\n",
    "\n",
    "In our case we will read that information from the environment variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_AI_KEY = os.environ[\"AZURE_AI_KEY\"]\n",
    "AZURE_AI_PROJECT = os.environ[\"AZURE_AI_PROJECT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ChatCompletionsClient(\n",
    "    endpoint=f\"https://{AZURE_AI_PROJECT}.services.ai.azure.com/models\",\n",
    "    credential=AzureKeyCredential(AZURE_AI_KEY),\n",
    "    api_version=\"2024-05-01-preview\",\n",
    "    model=\"Phi-4-mini-instruct\"\n",
    ")\n",
    "\n",
    "print(\" * AZURE AI (Phi-4 Mini Instruct) * \")\n",
    "run_inference(client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final example bootstraps a `ChatCompletionsClient` pointing at the local completion server from Foundry Local. In this case, we do not need to supply the credentials as the server is running locally and we can access it without authentication.\n",
    "\n",
    "Foundry Local exposes an OpenAI-compatible REST API, so it is a plug-and-play replacement for any OpenAI or Azure OpenAI endpoint.\n",
    "In my case, I configured Foundry Local to use `phi-4-mini-instruct`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ChatCompletionsClient(\n",
    "    endpoint=\"http://localhost:54342/v1\",\n",
    "    credential=AzureKeyCredential(\"\"),\n",
    "    model=\"Phi-4-mini-instruct-generic-gpu:4\"\n",
    ")\n",
    "\n",
    "print(\" * FOUNDRY LOCAL (Phi-4 Mini Instruct) * \")\n",
    "run_inference(client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS!\n",
    "\n",
    "The final example demonstrates running inference directly using MLX - Apple's machine learning framework for local model execution on Apple Silicon. This approach eliminates the need for any HTTP server and runs the model directly in-process.\n",
    "\n",
    "MLX is optimized for Apple Silicon and provides efficient local inference. In this case, we'll use the same `phi-4-mini-instruct` model but running it entirely locally without any API calls. The MLX libraries (`mlx-lm`) need to be installed separately.\n",
    "\n",
    "This example reuses the same prompt and user inputs, showing a seamless transition from cloud APIs → local server APIs → direct local inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mlx_lm.utils import load\n",
    "from mlx_lm.generate import generate\n",
    "from mlx_lm.sample_utils import make_sampler\n",
    "from huggingface_hub.utils.tqdm import disable_progress_bars\n",
    "\n",
    "# suppress noisy hugging face stuff\n",
    "disable_progress_bars()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "MODEL_PATH = \"mlx-community/Phi-4-mini-instruct-8bit\"\n",
    "model, tokenizer = load(MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" * MLX LOCAL (Phi-4 Mini Instruct) * \")\n",
    "\n",
    "for user_input in user_inputs:\n",
    "    messages = [{\"role\": \"user\", \"content\": f\"{instruction}{user_input}\"}]\n",
    "    print(f\"{user_input} -> \", end=\"\")\n",
    "    sampler = make_sampler(0.0)\n",
    "    prompt_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    \n",
    "    response = generate(model, tokenizer, prompt=prompt_text, sampler=sampler, max_tokens=50, verbose=False)\n",
    "    \n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
